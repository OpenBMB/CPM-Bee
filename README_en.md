
<div align="center">

# CPM-Bee

**Chinese-English Bilingual Foundation Model with 10 Billion Parameters**


<p align="center">
  <a href="#Model">Model</a> â€¢
  <a href="#Pre-training">OpenBMB</a> â€¢
  <a href="#Zero-shot">Performance</a> â€¢
  <a href="#modellicense">License</a>
</p>

</div>


**CPM-Bee** is a fully open-source, commercially-usable Chinese-English bilingual base model with a capacity of one hundred billion parameters. It is the second milestone achieved through the training process of [**CPM-live**](https://live.openbmb.org/).
Utilizing the Transformer auto-regressive architecture, CPM-Bee has been pre-trained on an extensive corpus of over 3 trillion high-quality tokens, thereby possessing remarkable foundational capabilities.



## âœ¨ Features

- **ğŸ‘ Open-source and Commercial Usable**ï¼šOpenBMB adheres to the spirit of open-source, aiming to make large-scale models accessible to everyone. CPM-Bee, as a foudation model, is fully open-source and available for commercial use, contributing to the advancement of the field of large-scale models.

- **ğŸ’« Excellent Performance in Chinese and English**ï¼š : CPM-Bee's base model has undergone rigorous selection and balancing of pre-training data, resulting in outstanding performance in both Chinese and English. For detailed information regarding evaluation tasks and results, please refer to the assessment documentation.


- **ğŸ“– Vast and High-quality Corpus**ï¼š CPM-Bee, as a base model, has been trained on an extensive corpus of over 3 trillion tokens, making it one of the models with the highest volume of training data within the open-source community. Furthermore, we have implemented stringent selection, cleaning, and post-processing procedures on the pre-training corpus to ensure its quality.

- **<img src="https://i.imgloc.com/2023/05/21/V4nLS3.png" width="20px"> Support for OpenBMB System**ï¼š The OpenBMB system provides a comprehensive ecosystem of tools and scripts for high-performance pre-training, adaptation, compression, deployment, and tool development. CPM-Bee, as a base model, is accompanied by all the necessary tool scripts, enabling developers to efficiently utilize and explore advanced functionalities.


- **ğŸ”¨ Conversational and Tool Usage Capabilities**ï¼š Building upon OpenBMB's exploration in instruction-based fine-tuning and tool learning, we have performed fine-tuning on top of the CPM-Bee base model, resulting in an instance model with powerful conversational and tool usage capabilities. The API and beta testing for this model will be made available in the near future.

## ğŸš€ Setup and Use

Clone the CPM-Bee repositoryï¼š
```bash
$ git clone -b master --single-branch https://github.com/OpenBMB/CPM-Bee.git
```
Please ensure the environment to meet the following requirements:
```bash
- python>=3.7
- torch>=1.10
```

We recommend using Anaconda to manage your environment and installing other dependencies from PyPI:
```bash
$ cd src
$ pip install -r requirements.txt
```

#### æ¨¡å‹

Model Link

- The CPM-Bee base model excels at accurate semantic understanding and efficiently handles various fundamental tasks, including text completion, text generation, translation, question answering, sentiment analysis, multiple-choice questions, and more.

```json
"å¡«ç©º":{"input": "å¿ƒç†å­¦é¢†åŸŸçš„ç ”ç©¶äººå‘˜å‘ç°ï¼Œåšå‡ºé‡è¦å†³å®šçš„æœ€å¥½æ–¹æ³•ä¹‹ä¸€ï¼Œæ¯”å¦‚é€‰æ‹©ä¸€æ‰€å¤§å­¦æˆ–<mask_0>ï¼Œéƒ½æ¶‰åŠåˆ°ä½¿ç”¨å†³ç­–å·¥ä½œè¡¨ã€‚ç ”ç©¶ä¼˜åŒ–çš„å¿ƒç†å­¦å®¶å°†<mask_1>ä¸ç†è®ºç†æƒ³å†³ç­–è¿›è¡Œæ¯”è¾ƒï¼Œçœ‹çœ‹å®ƒä»¬æœ‰å¤šç›¸ä¼¼ã€‚å·¥ä½œè¡¨ç¨‹åºçš„æ”¯æŒè€…è®¤ä¸ºå®ƒä¼šäº§ç”Ÿæœ€ä¼˜çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæœ€å¥½çš„å†³ç­–ã€‚è™½ç„¶æœ‰<mask_2>å¯ä»¥æ¥å—ï¼Œä½†å®ƒä»¬åœ¨æœ¬è´¨ä¸Šéƒ½æ˜¯ç›¸ä¼¼çš„ã€‚","<ans>":{"<mask_0>":"","<mask_1>":"","<mask_2>":""}},
"æ–‡æœ¬ç”Ÿæˆ": {"input": "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œæˆ‘å’Œå¦ˆå¦ˆä¸€èµ·å»å…¬å›­ï¼Œ<mask>", "prompt": "å¾€åå†™ä¸¤å¥è¯", "<ans>": ""}
"ç¿»è¯‘": {"input": "åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½", "prompt": "ä¸­ç¿»è‹±", "<ans>": ""}
"é—®ç­”": {"input": "NGC 6231æ˜¯ä¸€ä¸ªä½äºå¤©èåº§çš„ç–æ•£æ˜Ÿå›¢ï¼Œå¤©çƒåº§æ ‡ä¸ºèµ¤ç»16æ—¶54åˆ†ï¼Œèµ¤çº¬-41åº¦48åˆ†ï¼Œè§†è§‰è§‚æµ‹å¤§å°çº¦45è§’åˆ†ï¼Œäº®åº¦çº¦2.6è§†æ˜Ÿç­‰ï¼Œè·åœ°çƒ5900å…‰å¹´ã€‚NGC 6231å¹´é¾„çº¦ä¸ºä¸‰ç™¾äºŒåä¸‡å¹´ï¼Œæ˜¯ä¸€ä¸ªéå¸¸å¹´è½»çš„æ˜Ÿå›¢ï¼Œæ˜Ÿå›¢å†…çš„æœ€äº®æ˜Ÿæ˜¯5ç­‰çš„å¤©èåº§ Î¶1æ˜Ÿã€‚ç”¨åŒç­’æœ›è¿œé•œæˆ–å°å‹æœ›è¿œé•œå°±èƒ½çœ‹åˆ°ä¸ªåˆ«çš„è¡Œæ˜Ÿã€‚NGC 6231åœ¨1654å¹´è¢«æ„å¤§åˆ©å¤©æ–‡å­¦å®¶ä¹”ç“¦å°¼Â·å·´è’‚æ–¯ç‰¹Â·éœè¿ªå°”çº³ï¼ˆGiovanni Battista Hodiernaï¼‰ä»¥Luminosaeçš„åå­—é¦–æ¬¡çºªå½•åœ¨æ˜Ÿè¡¨ä¸­ï¼Œä½†æ˜¯æœªè§è®°è½½äºå¤å°”Â·æ¢…è¥¿è€¶çš„å¤©ä½“åˆ—è¡¨å’Œå¨å»‰Â·èµ«æ­‡å°”çš„æ·±ç©ºå¤©ä½“ç›®å½•ã€‚è¿™ä¸ªå¤©ä½“åœ¨1678å¹´è¢«çˆ±å¾·è’™Â·å“ˆé›·ï¼ˆI.7ï¼‰ã€1745å¹´è¢«å¤è¥¿äºšç§‘æ–¯ï¼ˆJean-Phillippe Loys de Cheseauxï¼‰ï¼ˆ9ï¼‰ã€1751å¹´è¢«å°¼å¯æ‹‰Â·è·¯æ˜“Â·æ‹‰å¡ä¼Šï¼ˆII.13ï¼‰åˆ†åˆ«å†æ¬¡ç‹¬ç«‹å‘ç°ã€‚", "question": "NGC 6231çš„ç»çº¬åº¦æ˜¯å¤šå°‘ï¼Ÿ", "<ans>": ""}
"è¯„åˆ†é¢„æµ‹": {"input":"ä¹‹å‰å¤šæ¬¡èšé¤éƒ½é€‰æ‹©è¿™é‡Œï¼Œæœ‰å„ç§å¤§å°çš„åŒ…æˆ¿åŒæ—¶èƒ½å®¹çº³å¾ˆå¤šäººï¼Œç¯å¢ƒå¥½æœ‰ç‰¹è‰²è¿˜æœ‰è¡¨æ¼”ï¼Œæ•´ä½“èšé¤æ°›å›´ä¸€ä¸‹è¢«å¸¦åŠ¨èµ·æ¥ã€‚ç°åœ¨ç”±äºç‚­ç«æ”¹æˆäº†ç”µçƒ¤ç¾Šï¼Œå£æ„ŸçœŸçš„ä¸å¦‚ä»å‰ï¼Œä¸è¿‡å…¶ä»–èœå“éƒ½è¿˜æ˜¯ä¸é”™ï¼Œçƒ¤ç¾Šå‰©ä¸‹çš„æ‹†éª¨è‚‰æœ€åè¿˜èƒ½å†åŠ å·¥ä¸€ä¸‹æ¤’ç›çš„ä¹Ÿå¾ˆå¥½åƒã€‚","question":"è¯„åˆ†æ˜¯å¤šå°‘ï¼Ÿ(1-5)","<ans>":""},
"é€‰æ‹©é¢˜": {"input": "çˆ¶æ¯éƒ½å¸Œæœ›è‡ªå·±çš„å­©å­è¯šå®ã€å‹‡æ•¢ã€æœ‰ç¤¼è²Œã€‚è¦æƒ³è®©å­©å­æˆä¸ºè¿™æ ·çš„äººï¼Œçˆ¶æ¯é¦–å…ˆå¾—ä»è‡ªå·±åšèµ·ï¼Œè¦æ˜¯è¿è‡ªå·±éƒ½åšä¸åˆ°ï¼Œåˆæ€èƒ½è¦æ±‚å­©å­åšåˆ°å‘¢ï¼Ÿ", "options": {"<option_0>": "å°‘æè¦æ±‚", "<option_1>": "é™ä½æ ‡å‡†", "<option_2>": "è‡ªå·±å…ˆåšå¥½", "<option_3>": "è®©å­©å­æ‹¿ä¸»æ„"}, "question": "æ•™è‚²å­©å­æ—¶ï¼Œçˆ¶æ¯åº”è¯¥ï¼š", "<ans>": ""}
```

## <img src="https://i.imgloc.com/2023/05/21/V4nLS3.png" width="25px"> OpenBMB

Leveraging the ecosystem of the OpenBMB large model system, we have implemented an efficient end-to-end training process for CPM-Bee. Additionally, we provide a complete set of scripts for various tasks, including continued training (using BMTrain), fine-tuning (using OpenPrompt and OpenDelta), tool usage (using BMTools), model compression (using BMCook), and efficient inference (using BMinf). These scripts are designed to assist developers in quickly getting started with and utilizing CPM-Bee effectively.

### Pre-training

We provide a pre-training [script](https://github.com/OpenBMB/CPM-Bee/blob/main/src/pretrain_cpm_bee.py) based on [BMTrain](https://github.com/OpenBMB/BMTrain) to improve the efficiency of training.


### Fine-tuning

Based on [OpenDelta](https://github.com/thunlp/OpenDelta), we provide two solutions of model tuning: full-parameter fine-tuning and parameter-efficient delta tuning, which could adapt CPM-Bee to various of scenarios.


1. Full-parameter fine-tuning:
```bash
$ torchrun --nnodes=1 --nproc_per_node=4 --rdzv_id=1 --rdzv_backend=c10d --rdzv_endpoint=localhost:12345 finetune_cpm_bee.py
```

2. Parameter-efficient delta tuningï¼š
```bash
$ torchrun --nnodes=1 --nproc_per_node=4 --rdzv_id=1 --rdzv_backend=c10d --rdzv_endpoint=localhost:12345 finetune_cpm_bee.py \
--use-delta \
```

#### Procedure


To fine-tune the model on a specific task, you should prepare the dataset and follow the steps below:

- Reshape the data format:

If you have a classification problem, you can integrate it into the format of multiple-choice questions. For more information on data formatting, you can refer to the CPM-Bee data format guidelines.
- Preprocess the dataset into binary files:

To construct a preprocessed dataset, you can execute the necessary preprocessing steps.

```bash
$ python preprocess_dataset.py --input your/reformated/data/path --output_path your/binary/data/path --output_name data_name
```
After processing, you will obtain
```bash
|-- your/binary/data/path
    |-- folder1
    |    |-- data_name
    |    |-- meta.bin
    |-- folder2
         |-- data_name
         |-- meta.bin
```

- Fine-tune CPM-Bee, run
``` bash
$ bash scripts/finetune_cpm_bee.sh
```
Alternatively, you can directly run `finetune_cpm_bee.py` using `torchrun`. For example, you can fine-tune CPM-Bee on a server with 4 GPUs as shown below:
```bash
torchrun --nnodes=1 --nproc_per_node=4 --rdzv_id=1 --rdzv_backend=c10d --rdzv_endpoint=localhost:12345 finetune_cpm_bee.py \
--model-config your/model/config/path \
--load your/model/checkpoint/path \
--dataset your/binary/data/path/folder1 \
--eval_dataset your/binary/data/path/folder2 \
--use-delta 
```


### Model Compression

Based on [BMCook](https://github.com/OpenBMB/BMCook), we have compressed the original CPM-Bee base model, offering multiple sizes of CPM-Bee models to accommodate various scenarios.



| Model         | #Attn.Layer | #FFN Layer| Attn Hidden Size | FFN Hidden Size | Download                                       |
| ----------- | ------- | ----- | --------- | -------- | ---------------------------------------- |
| CPM-Bee-10B | 48      | 48    | 4096      | 10240    |                                          |
| CPM-Bee-5B  | 19      | 24    | 4096      | 10240    | [Link](https://openbmb.oss-cn-hongkong.aliyuncs.com/model_center/cpm-bee-5b/cpm-bee-5b.zip) |
| CPM-Bee-2B  | 19      | 24    | 2048      | 5120     | [Link](https://openbmb.oss-cn-hongkong.aliyuncs.com/model_center/cpm-bee-2b/cpm-bee-2b.zip) |
| CPM-Bee-1B  | 19      | 24    | 1280      | 1024     | [Link](https://openbmb.oss-cn-hongkong.aliyuncs.com/model_center/cpm-bee-1b/cpm-bee-1b.zip) |



### Deployment


For the compressed CPM-Bee models, regular consumer-grade GPUs are sufficient for fast inference. The resource utilization for different sizes of models is as follows:

| Model          | Memory | Device           |
| ----------- | ------ | -------------- |
| CPM-Bee-10B | 20GB   | RTX3090ï¼ˆ24 GBï¼‰ |
| CPM-Bee-5B  | 11 GB  | RTX3090ï¼ˆ24 GBï¼‰ |
| CPM-Bee-2B  | 6.7 GB | GTX 1080ï¼ˆ8 GBï¼‰ |
| CPM-Bee-1B  | 4.1 GB | GTX 1660ï¼ˆ6 GBï¼‰ |




```python
from cpm_live.generation.bee import CPMBeeBeamSearch
from cpm_live.models import CPMBeeTorch, CPMBeeConfig
from cpm_live.tokenizers import CPMBeeTokenizer
from opendelta import LoraModel
import torch

prepare your input data.
data_list = [
    {"input": "ä»Šå¤©å¤©æ°”æ˜¯çœŸçš„<mask>", "prompt": "å¾€åå†™ä¸€å¥è¯", "<ans>": {"<mask>": ""}},
    {"input": "åŒ—äº¬å¸‚æ°”è±¡å°æç¤ºï¼Œ4æœˆ12æ—¥åˆååå—é£åŠ å¤§ï¼Œé˜µé£å¯è¾¾6çº§å·¦å³ï¼Œå—ä¸‹çš„æ²™å°˜å¯èƒ½ä¼´éšå›æµåŒ—ä¸Šè¿›äº¬ï¼Œå¤–å‡ºä»éœ€æ³¨æ„<mask_0>ï¼Œåšå¥½å¥åº·é˜²æŠ¤ã€‚å¤©æ´¥å¸‚æ°”è±¡å°ä¹Ÿæç¤ºï¼Œå—<mask_1>å½±å“ï¼Œæˆ‘å¸‚4æœˆ12æ—¥æœ‰æµ®å°˜å¤©æ°”ï¼ŒPM10æµ“åº¦<mask_2>ã€‚è¯·æ³¨æ„å…³å¥½é—¨çª—ï¼Œè€äººå„¿ç«¥å°½é‡å‡å°‘æˆ·å¤–æ´»åŠ¨ï¼Œå¤–å‡ºæ³¨æ„å¸¦å¥½<mask_3>ã€‚â€ ","<ans>":{"<mask_0>":"","<mask_1>":"","<mask_2>":"","<mask_3>":""}},
]

# load model
config = CPMBeeConfig.from_json_file("cpm-bee-5b.json")
ckpt_path = "cpm-bee-5b-ckpt.pt"
tokenizer = CPMBeeTokenizer()
model = CPMBeeTorch(config=config)

# insert LoRA
# delta_model = LoraModel(backbone_model=model, modified_modules=["project_q", "project_v"], backend="hf")

# load checkpoints
model.load_state_dict(torch.load(ckpt_path))
model.cuda()

# use beam search
beam_search = CPMBeeBeamSearch(
    model=model,
    tokenizer=tokenizer,
)
for data in data_list:
    inference_results = beam_search.generate([data], max_length=100)
    for res in inference_results:
        print(res)
# output:
# {'input': 'ä»Šå¤©å¤©æ°”æ˜¯çœŸçš„<mask>', 'prompt': 'å¾€åå†™ä¸€å¥è¯', '<ans>': {'<mask>': 'å¥½å•Šï¼'}}
# {'input': 'åŒ—äº¬å¸‚æ°”è±¡å°æç¤ºï¼Œ4æœˆ12æ—¥åˆååå—é£åŠ å¤§ï¼Œé˜µé£å¯è¾¾6çº§å·¦å³ï¼Œå—ä¸‹çš„æ²™å°˜å¯èƒ½ä¼´éšå›æµåŒ—ä¸Šè¿›äº¬ï¼Œå¤–å‡ºä»éœ€æ³¨æ„<mask_0>ï¼Œåšå¥½å¥åº·é˜²æŠ¤ã€‚å¤©æ´¥å¸‚æ°”è±¡å°ä¹Ÿæç¤ºï¼Œå—<mask_1>å½±å“ï¼Œæˆ‘å¸‚4æœˆ12æ—¥æœ‰æµ®å°˜å¤©æ°”ï¼ŒPM10æµ“åº¦<mask_2>ã€‚è¯·æ³¨æ„å…³å¥½é—¨çª—ï¼Œè€äººå„¿ç«¥å°½é‡å‡å°‘æˆ·å¤–æ´»åŠ¨ï¼Œå¤–å‡ºæ³¨æ„å¸¦å¥½<mask_3>ã€‚â€ ', '<ans>': {'<mask_0>': 'é˜²é£', '<mask_1>': 'æ²™å°˜å¤©æ°”', '<mask_2>': 'è¾ƒé«˜', '<mask_3>': 'å£ç½©'}}
```

We integrate the above code to a python file `text_generation.py`, which could be directly executed:
```bash
python text_generation.py
```
You can configure different input formats to accommodate different inference tasks.


## ğŸ’« Performance

### Zero-shot

We conducted comprehensive evaluations of the CPM-Bee base model's Chinese and English language capabilities. In the Chinese Zero-CLUE benchmark, CPM-Bee outperformed other models considerably, ranking first among large Chinese models. In the English benchmark, CPM-Bee demonstrated comparable performance to the open-source model LLaMA.

#### ZeroClue Chinese Evaluation

| **æ¨¡å‹**            | **EPRSTMT** | **CSLDCP** | **TNEWSF** | **IFLYTEKF** | **OCNLIF** | **BUSTM** | **CHIDF** | **CSLF**  | **CLUEWSCF** |
| ----------------- | ----------- | ---------- | ---------- | ------------ | ---------- | --------- | --------- | --------- | ------------ |
| **äºŒéƒç¥-UnifiedMC** | **88.71**   | 50.18      | 71.67      | 40.58        | 75.5       | 80.15     | 84.85     | 60.6      | 81.72        |
| **PaddleNLP-UTC** | 85.92       | **58.92**  | 68.27      | 40.15        | 74.79      | 76.7      | 82.75     | 70.6      | 74.48        |
| **å¤©ç¿¼äº‘**           | 87.25       | 48.02      | 77.13      | **59.62**    | 75.5       | **90.05** | 84.6      | 82.9      | 81.72        |
| **Mengzi-T5-MT**  | 68.93       | 86.99      | 55.19      | 74.73        | 22.42      | 74.69     | 77.6      | 85.1      | 84.17        |
| **CPM-Bee**       | 88.05       | 56.85      | **79.93**  | 58.85        | **81.28**  | 86.4      | **93.25** | **85.33** | **88.62**    |

#### English Evaluation



### CPM-Bee+ Decoder Tuning

Using the [Decoder Tuning](https://arxiv.org/abs/2212.08408) method developed jointly by OpenBMB and THUNLP (to be published at ACL 2023), it is possible to significantly improve the performance of downstream tasks solely through the use of APIs, without accessing or modifying the model parameters. This approach ensures both professionalism and fluency in achieving the desired results.



| **Shot** |     **Model**     |  **SST2** |  **IMDB** |  **Yelp** | **AGNews** | **DBpedia** | **Yahoo** |  **RTE**  |  **SNLI** | **MNLI-m** | **MNLI-mm** | **FewNERD** |  **Avg.** |
|----|-------------|-----|-----|-----|------|-------|-----|-----|-----|------|-------|-------|-----|
|   0   |    CPM-Bee    | 80.5  | 89.1  | 96.6  |  74.6  |  71.3   | 46.7  | 84.1  | 45.4  |  45.6  |  45.6   |   1.6   | 61.9  |
|  16  |     T5-3B     | 89.9  | 92.7  | 94.9  |  87.7  |  96.2   | 66.5  | 55.8  | 52.0  |  52.8  |  52.2   |  51.9   | 72.1  |
|      |    LLaMA-7B   | 85.1  | 90.5  | 92.8  |  71.4  |  89.8   | 45.1  | 49.1  | 35.2  |  36.3  |  36.2   |  54.6   | 62.4  |
|      |   Vicuna-13B  | 82.1  | 88.8  | 95.6  |  86.4  |  74.4   | 55.3  | 62.5  | 61.4  |  54.3  |  48.6   |  52.1   | 69.2  |
|      |    CPM-Bee    | 92.7  | 96.2  | 97.5  |  85.5  |  89.8   | 65.2  | 86.0  | 86.4  |  76.3  |  76.3   |  54.6   | **82.4**  |
|  64  |    LLaMA-7B   | 87.5  | 85.7  | 96.9  |  75.4  |  93.5   | 47.4  | 51.4  | 39.4  |  36.2  |  38.4   |  59.8   | 64.7  |
|      |   Vicuna-13B  | 92.0  | 90.8  | 96.5  |  87.7  |  87.8   | 58.7  | 59.1  | 58.7  |  56.7  |  48.4   |  56.8   | 72.1  |
|      |    CPM-Bee    | 94.3  | 96.5  | 98.3  |  88.5  |  93.5   | 68.7  | 87.1  | 88.9  |  78.0  |  79.0   |  59.8   | **84.8**  |
|  256 |    LLaMA-7B   | 87.6  | 88.8  | 97.1  |  82.4  |  94.2   | 48.5  | 53.4  | 39.8  |  37.3  |  37.4   |  59.1   | 66.0  |
|      |   Vicuna-13B  | 93.1  | 88.7  | 96.8  |  89.9  |  89.1   | 58.6  | 58.5  | 58.7  |  57.5  |  48.3   |  56.6   | 72.3  |
|      |    CPM-Bee    | 94.5  | 96.7  | 98.4  |  89.7  |  94.2   | 69.9  | 87.7  | 89.4  |  81.7  |  80.6   |  59.1   | **85.6**  |


## ğŸ“ƒLicense

#### Model License
The CPM-Bee base, governed by the [General Model License (GML)](https://www.openbmb.org/models/license), permits commercial usage. If you intend to utilize the model for commercial purposes, please reach out to business@modelbest.cn to obtain the official license.


#### Statement
As a language model, CPM-Bee generates content by learning from a vast amount of text. However, it does not possess the ability to comprehend or express personal opinions or value judgments. Any content generated by CPM-Bee does not represent the viewpoints or positions of the model developers.
Therefore, when using content generated by CPM-Bee, users should take full responsibility for evaluating and verifying it on their own

